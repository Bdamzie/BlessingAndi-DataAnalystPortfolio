{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {},
      "source": [
        "# initialize sql context\r\n",
        "from pyspark.sql import SQLContext\r\n",
        "\r\n",
        "sqlContext = SQLContext(sc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Todo: using SQLContext to read csv and assign to dataframe\r\n",
        "df = sqlContext.read.csv('abfss://synapsedatalake@synapseaiadadls.dfs.core.windows.net/pocdata.csv', header=True, inferSchema= True)\t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Todo:printSchema\r\n",
        "df =df.drop('CUSTID', 'Latest_TxnDate', 'State', 'LGA', 'Account_df.printSchema()Open_Date')\r\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "spark.sql(\"CREATE DATABASE IF NOT EXISTS demo\")\r\n",
        "df.write.mode(\"overwrite\").saveAsTable(\"demo.Churnmldata\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Import all from `sql.types`\r\n",
        "from pyspark.sql.types import *\r\n",
        "\r\n",
        "# Write a custom function to convert the data type of DataFrame columns\r\n",
        "def convertColumn(df, names, newType):\r\n",
        "    for name in names: \r\n",
        "        df = df.withColumn(name, df[name].cast(newType))\r\n",
        "    return df \r\n",
        "# List of continuous features\r\n",
        "CONTI_FEATURES  = ['No_of_Accounts', \r\n",
        "'GDP_in_Billions_of_USSD',\r\n",
        "'Inflation',\r\n",
        "'Population',\r\n",
        "'txn_amount_M1',\r\n",
        " 'txn_vol_M1',\r\n",
        " 'txn_amount_M2',\r\n",
        " 'txn_vol_M2',\r\n",
        " 'txn_amount_M3',\r\n",
        " 'txn_vol_M3',\r\n",
        " 'F1']\r\n",
        "# Convert the type\r\n",
        "df = convertColumn(df, CONTI_FEATURES, FloatType())\r\n",
        "# Check the dataset\r\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df.show(5, truncate = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#import libraries for pipeline\r\n",
        "from pyspark.ml import Pipeline\r\n",
        "from pyspark.ml.feature import OneHotEncoder\r\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 1. Encode the categorical data\r\n",
        "CATE_FEATURES = ['Gender',\r\n",
        "'Age_Band',\r\n",
        "'Tenure',\r\n",
        "'Education', \r\n",
        "'Marital_Status', \r\n",
        "'Segment', \r\n",
        "'Occupation', \r\n",
        "'F2', \r\n",
        "'Recency', \r\n",
        "'Freq_M1',\r\n",
        "'Freq_M2',\r\n",
        "'F3']\r\n",
        "# stages in our Pipeline\r\n",
        "stages = [] \r\n",
        "\r\n",
        "\r\n",
        "for categoricalCol in CATE_FEATURES:\r\n",
        "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\r\n",
        "    \r\n",
        "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()],\r\n",
        "                                     outputCols=[categoricalCol + \"classVec\"])\r\n",
        "    \r\n",
        "    stages += [stringIndexer, encoder]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 2. Index the label feature\r\n",
        "# Convert label into label indices using the StringIndexer\r\n",
        "CHURN_stringIdx =  StringIndexer(inputCol=\"CHURN\", outputCol=\"newCHURN\")\r\n",
        "stages += [CHURN_stringIdx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 3. Add continuous variable\r\n",
        "assemblerInputs = [c + \"classVec\" for c in CATE_FEATURES] + CONTI_FEATURES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 4. Assemble the steps\r\n",
        "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\r\n",
        "stages += [assembler]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "stages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Create a Pipeline.\r\n",
        "pipeline = Pipeline(stages=stages)\r\n",
        "pipelineModel = pipeline.fit(df)\r\n",
        "model = pipelineModel.transform(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# To make the computation faster, you convert model to a DataFrame.\r\n",
        "#You need to select newlabel and features from model using map.\r\n",
        "\r\n",
        "from pyspark.ml.linalg import DenseVector\r\n",
        "input_data = model.rdd.map(lambda x: (x[\"newCHURN\"], DenseVector(x[\"features\"])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# import \r\n",
        "# from pyspark.ml.linalg import DenseVector\r\n",
        "df_train = sqlContext.createDataFrame(input_data, [\"newCHURN\", \"features\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_train.show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Split the data into train and test sets\r\n",
        "train_data, test_data = df_train.randomSplit([.8,.2],seed=1234)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Let's count how many people with income below/above 50k in both training and test set\r\n",
        "train_data.groupby('newCHURN').agg({'newCHURN': 'count'}).show()\t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "test_data.groupby('newCHURN').agg({'newCHURN': 'count'}).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#You initialize lr by indicating the label column and feature columns. \r\n",
        "# Import `LogisticRegression`\r\n",
        "from pyspark.ml.classification import LogisticRegression\r\n",
        "\r\n",
        "# Initialize `lr`\r\n",
        "lr = LogisticRegression(labelCol=\"newCHURN\",\r\n",
        "                        featuresCol=\"features\",\r\n",
        "                        maxIter=10,\r\n",
        "                        regParam=0.3)\r\n",
        "\r\n",
        "# Fit the data to the model\r\n",
        "clr = linearModel = lr.fit(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Print the coefficients and intercept for logistic regression\r\n",
        "print(\"Coefficients: \" + str(linearModel.coefficients))\r\n",
        "print(\"Intercept: \" + str(linearModel.intercept))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#To generate prediction for your test set, you can use linearModel with transform() on test_data\r\n",
        "# Make predictions on test data using the transform() method.\r\n",
        "predictions = linearModel.transform(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "predictions.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "selected = predictions.select(\"newCHURN\", \"prediction\", \"probability\")\r\n",
        "selected.show(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#We need to look at the accuracy metric to see how well (or bad) the model performs.\r\n",
        "\r\n",
        "def accuracy_m(model): \r\n",
        "    predictions = model.transform(test_data)\r\n",
        "    cm = predictions.select(\"newCHURN\", \"prediction\")\r\n",
        "    acc = cm.filter(cm.newCHURN == cm.prediction).count() / cm.count()\r\n",
        "    print(\"Model accuracy: %.3f%%\" % (acc * 100)) \r\n",
        "\r\n",
        "accuracy_m(model = linearModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Use ROC \r\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\r\n",
        "\r\n",
        "# Evaluate model\r\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"newCHURN\")\r\n",
        "print(evaluator.evaluate(predictions))\r\n",
        "print(evaluator.getMetricName())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#you can tune the hyperparameters. \r\n",
        "#Similar to scikit learn you create a parameter grid, and you add the parameters you want to tune. \r\n",
        "#To reduce the time of the computation, you only tune the regularization parameter with only two values.\r\n",
        "#use \r\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\r\n",
        "\r\n",
        "# Create ParamGrid for Cross Validation\r\n",
        "paramGrid = (ParamGridBuilder()\r\n",
        "             .addGrid(lr.regParam, [0.01, 0.5])\r\n",
        "             .build())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from time import *\r\n",
        "start_time = time()\r\n",
        "\r\n",
        "# Create 5-fold CrossValidator\r\n",
        "cv = CrossValidator(estimator=lr,\r\n",
        "                    estimatorParamMaps=paramGrid,\r\n",
        "                    evaluator=evaluator, numFolds=5)\r\n",
        "\r\n",
        "# Run cross validations\r\n",
        "cvModel = cv.fit(train_data)\r\n",
        "# likely take a fair amount of time\r\n",
        "end_time = time()\r\n",
        "elapsed_time = end_time - start_time\r\n",
        "print(\"Time to train model: %.3f seconds\" % elapsed_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#accuracy of cv selected model\r\n",
        "accuracy_m(model = cvModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#We can exctract the recommended parameter by chaining cvModel.bestModel with extractParamMap()\r\n",
        "\r\n",
        "bestModel = cvModel.bestModel\r\n",
        "bestModel.extractParamMap()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from onnxmltools.convert.common.data_types import StringTensorType\r\n",
        "from onnxmltools.convert.common.data_types import FloatTensorType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "initial_types = [\r\n",
        "    (\"CHURN\", StringTensorType([1, 1])),\r\n",
        "    (\"Gender\", StringTensorType([1, 1])), \r\n",
        "    (\"Age_Band\", StringTensorType([1, 1])), \r\n",
        "    (\"Tenure\", StringTensorType([1, 1])), \r\n",
        "    (\"Occupation\", StringTensorType([1, 1])), \r\n",
        "    (\"Education\", StringTensorType([1, 1])), \r\n",
        "    (\"Segment\", StringTensorType([1, 1])), \r\n",
        "    (\"Marital_Status\", StringTensorType([1, 1])), \r\n",
        "    (\"No_of_Accounts\", FloatTensorType([1, 1])), \r\n",
        "    (\"GDP_in_Billions_of_USSD\", FloatTensorType([1, 1])),\r\n",
        "    (\"Inflation\", FloatTensorType([1, 1])), \r\n",
        "    (\"Population\", FloatTensorType([1, 1])), \r\n",
        "    (\"txn_amount_M1\", FloatTensorType([1, 1])), \r\n",
        "    (\"txn_vol_M1\", FloatTensorType([1, 1])), \r\n",
        "    (\"txn_amount_M2\", FloatTensorType([1, 1])), \r\n",
        "    (\"txn_vol_M2\", FloatTensorType([1, 1])), \r\n",
        "    (\"txn_amount_M3\", FloatTensorType([1, 1])), \r\n",
        "    (\"txn_vol_M3\", FloatTensorType([1, 1])), \r\n",
        "    (\"F1\", FloatTensorType([1, 1])), \r\n",
        "    (\"F2\", StringTensorType([1, 1])), \r\n",
        "    (\"Recency\", StringTensorType([1, 1])), \r\n",
        "    (\"Freq_M1\", StringTensorType([1, 1])), \r\n",
        "    (\"Freq_M2\", StringTensorType([1, 1])), \r\n",
        "    (\"F3\", StringTensorType([1, 1]))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from onnxmltools import convert_sparkml\r\n",
        "from onnxmltools.utils import save_model\r\n",
        "model_onnx = convert_sparkml(pipelineModel, 'churn prediction model', initial_types)\r\n",
        "model_onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with open(\"model.onnx\", \"wb\") as f:\r\n",
        "    f.write(model_onnx.SerializeToString())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "connection_string = \"DefaultEndpointsProtocol=https;AccountName=synapseaiadadls;AccountKey=6qaIFdm8bVwvLMRgpdDWRl4pOWK6qS2p0MuFYISrs6TdX4MxWxLpUXd3Lm33PidhWo/P5jU+Ws5FmLQS3t6+Ww==;EndpointSuffix=core.windows.net\"\r\n",
        "from azure.storage.blob import BlobClient\r\n",
        "blob = BlobClient.from_connection_string(conn_str=connection_string, container_name=\"models\",blob_name=\"onnx/model.onnx\")\r\n",
        "\r\n",
        "with open(\"./model.onnx\", \"rb\") as data:\r\n",
        "    blob.upload_blob(data, overwrite = True)"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}