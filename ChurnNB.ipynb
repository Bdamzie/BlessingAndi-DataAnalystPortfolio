{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "from datetime import datetime\r\n",
        "from dateutil import parser\r\n",
        "from pyspark.sql.functions import unix_timestamp, date_format, col, when\r\n",
        "from pyspark.ml import Pipeline\r\n",
        "from pyspark.ml import PipelineModel\r\n",
        "from pyspark.ml.feature import RFormula\r\n",
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer\r\n",
        "from pyspark.ml.classification import LogisticRegression\r\n",
        "from pyspark.mllib.evaluation import BinaryClassificationMetrics\r\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df = spark.read.csv('abfss://synapsedatalake@synapseaiadadls.dfs.core.windows.net/pocdata.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql.functions import col, desc\r\n",
        "\r\n",
        "df = spark.read.option(\"header\", \"true\").csv('abfss://synapsedatalake@synapseaiadadls.dfs.core.windows.net/pocdata.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df.createOrReplaceTempView(\"pocdata\")\r\n",
        "pocdata = df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "pocdata.show"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "sI1 = StringIndexer(inputCol=\"Gender\", outputCol=\"GenderIndex\")\r\n",
        "en1 = OneHotEncoder(dropLast=False, inputCol=\"GenderIndex\", outputCol=\"GenderVec\")\r\n",
        "sI2 = StringIndexer(inputCol=\"Segment\", outputCol=\"SegmentIndex\")\r\n",
        "en2 = OneHotEncoder(dropLast=False, inputCol=\"SegmentIndex\", outputCol=\"SegmentVec\")\r\n",
        "\r\n",
        "encoded_final_df = Pipeline(stages=[sI1, en1, sI2, en2]).fit(pocdata).transform(pocdata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Decide on the split between training and testing data from the DataFrame\r\n",
        "trainingFraction = 0.7\r\n",
        "testingFraction = (1-trainingFraction)\r\n",
        "seed = 1234\r\n",
        "\r\n",
        "# Split the DataFrame into test and training DataFrames\r\n",
        "train_data_pocdata, test_data_pocdata = encoded_final_df.randomSplit([trainingFraction, testingFraction], seed=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Create a new logistic regression object for the model\r\n",
        "logReg = LogisticRegression(maxIter=10, regParam=0.3, labelCol = 'CHURN')\r\n",
        "\r\n",
        "## The formula for the model\r\n",
        "classFormula = RFormula(formula=\"CHURN ~ CUSTID + GenderVec + Age_Band + State + LGA + Account_Open_Date + Tenure + Occupation + Education + SegmentVec + Marital_Status + No_of_Accounts + GDP_in_Billions_of_USSD + Inflation + Population + txn_amount_M1 + txn_vol_M1 + txn_amount_M2 + txn_vol_M2 + txn_amount_M3 + txn_vol_M3 + F1 + F2 + Latest_TxnDate + Recency + Freq_M1 + Freq_M2 + F3\")\r\n",
        "\r\n",
        "## Undertake training and create a logistic regression model\r\n",
        "lrModel = Pipeline(stages=[classFormula, logReg]).fit(train_data_df)\r\n",
        "\r\n",
        "## Saving the model is optional, but it's another form of inter-session cache\r\n",
        "datestamp = datetime.now().strftime('%m-%d-%Y-%s')\r\n",
        "fileName = \"lrModel_\"  +  datestamp\r\n",
        "logRegDirfilename = fileName\r\n",
        "lrModel.save(logRegDirfilename)\r\n",
        "\r\n",
        "## Predict tip 1/0 (yes/no) on the test dataset; evaluation using area under ROC\r\n",
        "predictions = lrModel.transform(test_data_df)\r\n",
        "predictionAndLabels = predictions.select(\"label\",\"prediction\").rdd\r\n",
        "metrics = BinaryClassificationMetrics(predictionAndLabels)\r\n",
        "print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}