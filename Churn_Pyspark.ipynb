{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {},
      "source": [
        "# initialize sql context\r\n",
        "from pyspark.sql import SQLContext\r\n",
        "\r\n",
        "sqlContext = SQLContext(sc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Todo: using SQLContext to read csv and assign to dataframe\r\n",
        "df = sqlContext.read.csv('abfss://synapsedatalake@synapseaiadadls.dfs.core.windows.net/pocdata.csv', header=True, inferSchema= True)\t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Todo:printSchema\r\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Run the cell to rename the columns properly:\r\n",
        "cols = ['CUSTID','Gender','Age_Band','State','LGA','Account_Open_Date','Tenure','Occupation','Education','Segment','Marital','No_of_Accounts','GDP_in_Billions_of_USSD','Inflation','Population','txn_amount_M1','txn_vol_M1','txn_amount_M2','txn_vol_M2','txn_amount_M3','txn_vol_M3','F1','F2','Latest_TxnDate','Recency','Freq_M1','Freq_M2','F3','CHURN']\r\n",
        "\r\n",
        "#note income -renamed-> as label\r\n",
        "df=df.toDF(*cols)\r\n",
        "\r\n",
        "         "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Import all from `sql.types`\r\n",
        "from pyspark.sql.types import *\r\n",
        "\r\n",
        "# Write a custom function to convert the data type of DataFrame columns\r\n",
        "def convertColumn(df, names, newType):\r\n",
        "    for name in names: \r\n",
        "        df = df.withColumn(name, df[name].cast(newType))\r\n",
        "    return df \r\n",
        "# List of continuous features\r\n",
        "CONTI_FEATURES  = ['GDP_in_Billions_of_USSD','Inflation','Population','txn_amount_M1','txn_vol_M1','txn_amount_M2','txn_vol_M2','txn_amount_M3','txn_vol_M3','F1']\r\n",
        "# Convert the type\r\n",
        "df = convertColumn(df, CONTI_FEATURES, FloatType())\r\n",
        "# Check the dataset\r\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df.show(5, truncate = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#todo: describe data,column by count,by group\r\n",
        "df.describe().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# the summary statistic of only one column, add the name of the column inside describe()\r\n",
        "\r\n",
        "df.describe('txn_vol_m1').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Descriptive statistics by group\r\n",
        "df.groupBy(\"Marital\").count().sort(\"count\",ascending=True).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#example\r\n",
        "df.groupby('Age_Band').agg({'txn_vol_m2': 'mean'}).show()\t\t\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#todo crosstab computation\r\n",
        "df.crosstab('Age_Band', 'CHURN').sort(\"Age_Band_CHURN\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Drop null vals\r\n",
        "#df  = df.drop('Population', 'Inflation','State','LGA')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#TODO:  # Add age square\r\n",
        "        \r\n",
        "#from pyspark.sql.functions import *\r\n",
        "\r\n",
        "# 1 Select the column\r\n",
        "#age_square = df.select(col(\"age\")**2)\r\n",
        "\r\n",
        "# 2 Apply the transformation and add it to the DataFrame\r\n",
        "#df = df.withColumn(\"age_square\", col(\"age\")**2)\r\n",
        "\r\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#TODO : follow the above instruction\r\n",
        "#df.filter(df.Segment == 'HNI').count()\r\n",
        "#df.groupby('Segment').agg({'Segment': 'count'}).sort(asc(\"count(Segment)\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_remove = df.filter(df.Recency!='B')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#import libraries for pipeline\r\n",
        "from pyspark.ml import Pipeline\r\n",
        "from pyspark.ml.feature import OneHotEncoder\r\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 1. Encode the categorical data\r\n",
        "CATE_FEATURES = ['Gender','Education', 'Marital', 'Segment', 'Occupation', 'F2', 'Recency', 'Freq_M1','Freq_M2','F3']\r\n",
        "\r\n",
        "# stages in our Pipeline\r\n",
        "stages = [] \r\n",
        "\r\n",
        "\r\n",
        "for categoricalCol in CATE_FEATURES:\r\n",
        "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\r\n",
        "    \r\n",
        "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()],\r\n",
        "                                     outputCols=[categoricalCol + \"classVec\"])\r\n",
        "    \r\n",
        "    stages += [stringIndexer, encoder]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 2. Index the label feature\r\n",
        "# Convert label into label indices using the StringIndexer\r\n",
        "label_stringIdx =  StringIndexer(inputCol=\"CHURN\", outputCol=\"newCHURN\")\r\n",
        "stages += [label_stringIdx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 3. Add continuous variable\r\n",
        "assemblerInputs = [c + \"classVec\" for c in CATE_FEATURES] + CONTI_FEATURES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 4. Assemble the steps\r\n",
        "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\r\n",
        "stages += [assembler]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "stages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Create a Pipeline.\r\n",
        "pipeline = Pipeline(stages=stages)\r\n",
        "pipelineModel = pipeline.fit(df_remove)\r\n",
        "model = pipelineModel.transform(df_remove)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# To make the computation faster, you convert model to a DataFrame.\r\n",
        "#You need to select newlabel and features from model using map.\r\n",
        "\r\n",
        "from pyspark.ml.linalg import DenseVector\r\n",
        "input_data = model.rdd.map(lambda x: (x[\"newCHURN\"], DenseVector(x[\"features\"])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# import \r\n",
        "# from pyspark.ml.linalg import DenseVector\r\n",
        "df_train = sqlContext.createDataFrame(input_data, [\"CHURN\", \"features\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_train.show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Split the data into train and test sets\r\n",
        "train_data, test_data = df_train.randomSplit([.8,.2],seed=1234)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Let's count how many people with income below/above 50k in both training and test set\r\n",
        "train_data.groupby('CHURN').agg({'CHURN': 'count'}).show()\t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "test_data.groupby('CHURN').agg({'CHURN': 'count'}).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#You initialize lr by indicating the label column and feature columns. \r\n",
        "# Import `LogisticRegression`\r\n",
        "from pyspark.ml.classification import LogisticRegression\r\n",
        "\r\n",
        "# Initialize `lr`\r\n",
        "lr = LogisticRegression(labelCol=\"CHURN\",\r\n",
        "                        featuresCol=\"features\",\r\n",
        "                        maxIter=10,\r\n",
        "                        regParam=0.3)\r\n",
        "\r\n",
        "# Fit the data to the model\r\n",
        "linearModel = lr.fit(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Print the coefficients and intercept for logistic regression\r\n",
        "print(\"Coefficients: \" + str(linearModel.coefficients))\r\n",
        "print(\"Intercept: \" + str(linearModel.intercept))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#To generate prediction for your test set, you can use linearModel with transform() on test_data\r\n",
        "# Make predictions on test data using the transform() method.\r\n",
        "predictions = linearModel.transform(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "predictions.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "selected = predictions.select(\"CHURN\", \"prediction\", \"probability\")\r\n",
        "selected.show(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#We need to look at the accuracy metric to see how well (or bad) the model performs.\r\n",
        "\r\n",
        "def accuracy_m(model): \r\n",
        "    predictions = model.transform(test_data)\r\n",
        "    cm = predictions.select(\"CHURN\", \"prediction\")\r\n",
        "    acc = cm.filter(cm.CHURN == cm.prediction).count() / cm.count()\r\n",
        "    print(\"Model accuracy: %.3f%%\" % (acc * 100)) \r\n",
        "\r\n",
        "accuracy_m(model = linearModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}