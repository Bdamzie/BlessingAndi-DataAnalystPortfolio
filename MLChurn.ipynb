{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 32,
      "outputs": [],
      "metadata": {},
      "source": [
        "#Loading Libraries\r\n",
        "\r\n",
        "#import matplotlib.pyplot as plt\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.ml import Pipeline\r\n",
        "from pyspark.sql.functions import mean, col, split, col, regexp_extract, when, lit\r\n",
        "from pyspark.sql.types import *\r\n",
        "from pyspark.ml.feature import StringIndexer, IndexToString, VectorAssembler, VectorIndexer\r\n",
        "from pyspark.ml.feature import QuantileDiscretizer, OneHotEncoder\r\n",
        "\r\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\r\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\r\n",
        "\r\n",
        "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier\r\n",
        "\r\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\r\n",
        "\r\n",
        "from datetime import datetime\r\n",
        "from dateutil import parser\r\n",
        "from pyspark.mllib.evaluation import BinaryClassificationMetrics\r\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "spark = SparkSession.builder.appName(\"Spark ML applied on Churn POC dataset\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Get Data\r\n",
        "# Primary storage info\r\n",
        "#account_name = 'synapseaiadadls'# primary ADLS account name https://synapseaiadadls.dfs.core.windows.net.\r\n",
        "#container_name = 'root' # Primary ADLS Gen2 file system from Synapse Home Page\r\n",
        "#relative_path = 'Raw' # relative folder path\r\n",
        "#filename = 'pocdata.csv'\r\n",
        "#poc_data_path = 'abfss://%s@%s.dfs.core.windows.net/%s/%s' % (container_name, account_name, relative_path, filename)\r\n",
        "poc_data_path = 'abfss://synapsedatalake@synapseaiadadls.dfs.core.windows.net/pocdata.csv'\r\n",
        "\r\n",
        "poc_df = spark.read.csv(poc_data_path, header = 'True', inferSchema = 'True')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "poc_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "poc_df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "spark.sql(\"CREATE DATABASE IF NOT EXISTS devtest\")\r\n",
        "poc_df.write.mode(\"overwrite\").saveAsTable(\"devtest.Mlchurndata\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Number of cutomers in the dataframe\r\n",
        "\r\n",
        "customers_count = poc_df.count()\r\n",
        "print(\"Number of cutomers is {}\".format(customers_count))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "groupBy_customers = poc_df.groupBy(\"churn\").count()\r\n",
        "\r\n",
        "groupBy_customers.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "display(groupBy_customers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Summary statistics for the numeric variables\r\n",
        "poc_df.describe([t[0] for t in poc_df.dtypes if t[1] == 'int']).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "display(poc_df.groupBy(\"segment\").count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Process Data\r\n",
        "def get_dummy(df, categoricalCols, continuousCols, labelCol):\r\n",
        "  \r\n",
        "  indexers = [StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c)) for c in categoricalCols]\r\n",
        "\r\n",
        "  encoders = [OneHotEncoder(inputCol=indexer.getOutputCol(),\r\n",
        "                             outputCol=\"{0}_encoded\".format(indexer.getOutputCol()))\r\n",
        "              for indexer in indexers]\r\n",
        "\r\n",
        "  assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders]\r\n",
        "                              + continuousCols, outputCol=\"features\")\r\n",
        "  \r\n",
        "  indexer = StringIndexer(inputCol=labelCol, outputCol='indexedLabel')\r\n",
        "\r\n",
        "  pipeline = Pipeline(stages = indexers + encoders + [assembler] + [indexer])\r\n",
        "\r\n",
        "  model=pipeline.fit(df)\r\n",
        "  data = model.transform(df)\r\n",
        "\r\n",
        "  data = data.withColumn('label', col(labelCol))\r\n",
        "  \r\n",
        "  return data.select('features', 'indexedLabel', 'label'), StringIndexer(inputCol='label').fit(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Transform Data: Deal with categorical data and Convert the data to dense vector\r\n",
        "categoricalColumns = ['Gender', 'Age_Band', 'Tenure', 'Education', 'Marital_Status', 'Segment', 'Occupation', 'F2', 'Recency', 'Freq_M1', 'Freq_M2', 'F3']\r\n",
        "numericColumns = ['No_of_Accounts', 'GDP_in_Billions_of_USSD', 'Inflation', 'Population', 'txn_amount_M1', 'txn_vol_M1', 'txn_amount_M2', 'txn_vol_M2', 'txn_amount_M3', 'txn_vol_M3', 'F1']\r\n",
        "(poc_df, labelindexer) = get_dummy(poc_df, categoricalColumns, numericColumns, 'CHURN')\r\n",
        "poc_df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Deal with Categorical Label and Variables\r\n",
        "# Identify Categorical features and index them\r\n",
        "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(poc_df)\r\n",
        "\r\n",
        "featureIndexer.transform(poc_df).show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#show\r\n",
        "poc_df.show(5, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Create Split and Training Datasets\r\n",
        "(trainingData, testData) = poc_df.randomSplit([0.8, 0.2], seed=10)\r\n",
        "print(\"Training Dataset Count: \" + str(trainingData.count()))\r\n",
        "print(\"Test Dataset Count: \" + str(testData.count()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#show\r\n",
        "print(\"The first 5 samples of the Training Dataset:\")\r\n",
        "trainingData.show(5, False)\r\n",
        "print(\"The first 5 samples of the Test Dataset:\")\r\n",
        "testData.show(5, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Fit & Evaluate Logistic Regression\r\n",
        "#lr = LogisticRegression(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\") #using this line if you would using indexedFeatures instead features\r\n",
        "lr = LogisticRegression(labelCol=\"indexedLabel\", featuresCol=\"features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Pipeline\r\n",
        "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=labelindexer.labels) \r\n",
        "\r\n",
        "pipeline = Pipeline(stages=[featureIndexer, lr, labelConverter])\r\n",
        "\r\n",
        "lrModel = pipeline.fit(trainingData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Predict: Make predictions on the test data using the transform() method. \r\n",
        "#LogisticRegression.transform() will only use the column given in featuresCol parameter. \r\n",
        "predictions = lrModel.transform(testData)\r\n",
        "\r\n",
        "predictions.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "predictions.select(\"features\", \"label\", \"probability\", \"predictedLabel\").show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Compute the model accuracy\r\n",
        "cm = predictions.select(\"label\", \"predictedLabel\")\t\t\t\r\n",
        "cm.groupby('label').agg({'label': 'count'}).show()\t\r\n",
        "cm.groupby('predictedLabel').agg({'predictedLabel': 'count'}).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "predictions.groupBy('label', 'predictedLabel').count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "print(\"The Accuracy for test set is {}\".format(cm.filter(cm.label == cm.predictedLabel).count()/cm.count()))\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Using the MulticlassClassificationEvaluator() function:\r\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\r\n",
        "print(\"The Accuracy for test set is {}\".format(evaluator.evaluate(predictions)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Evaluation\r\n",
        "#from pyspark.ml.evaluation import MulticlassClassificationEvaluator\r\n",
        "\r\n",
        "# Select (prediction, true label) and compute test error\r\n",
        "evaluator = MulticlassClassificationEvaluator(\r\n",
        "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\r\n",
        "accuracy = evaluator.evaluate(predictions)\r\n",
        "print(\"Test Error = %g\" % (1.0 - accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Create Confusion Matrix\r\n",
        "\r\n",
        "from pyspark.mllib.evaluation import BinaryClassificationMetrics\r\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\r\n",
        "\r\n",
        "predictionAndLabel = predictions.select(\"prediction\", \"indexedLabel\").rdd\r\n",
        "\r\n",
        "# Instantiate metrics object \r\n",
        "metricsMulti = MulticlassMetrics(predictionAndLabel)\r\n",
        "metricsBinary= BinaryClassificationMetrics(predictionAndLabel)\r\n",
        "# Overall statistics \r\n",
        "confusionMatrix = metricsMulti.confusionMatrix()\r\n",
        "precision = metricsMulti.precision(label=1) \r\n",
        "recall = metricsMulti.recall(label=1) \r\n",
        "f1Score = metricsMulti.fMeasure(0.0,1.0) \r\n",
        "print(\"Summary Stats\")\r\n",
        "print(\"Confusion Matrix = \\n %s\" % confusionMatrix)\r\n",
        "print(\"Precision = %s\" % precision) \r\n",
        "print(\"Recall = %s\" % recall) \r\n",
        "print(\"F1 Score = %s\" % f1Score) \r\n",
        "\r\n",
        "# Area under precision-recall curve \r\n",
        "print(\"Area under PR = %s\" % metricsBinary.areaUnderPR) \r\n",
        "# Area under ROC curve \r\n",
        "print(\"Area under ROC = %s\" % metricsBinary.areaUnderROC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Compute the area under ROC metric\r\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",labelCol=\"indexedLabel\")\r\n",
        "print(\"The area under ROC for test set is {}\".format(evaluator.evaluate(predictions)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Model Tuning\r\n",
        "print(lr.explainParams())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Hyperparameter tuning using 5-fold cross validation\r\n",
        "#we indicate 3 values for regParam, 3 values for maxIter, and 3 values for elasticNetParam, this grid will have then 3 x 3 x 3 = 27 parameter settings for CrossValidator to choose from. \r\n",
        "#We will create a 5-fold CrossValidator.\r\n",
        "\r\n",
        "#Create ParamGrid for Cross Validation\r\n",
        "paramGrid = (ParamGridBuilder()\r\n",
        "             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\r\n",
        "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\r\n",
        "             .addGrid(lr.maxIter, [1, 5, 10])\r\n",
        "             .build())\r\n",
        "\r\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",labelCol=\"indexedLabel\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Create and run 5-fold CrossValidator\r\n",
        "#cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\r\n",
        "#pipeline = Pipeline(stages=[featureIndexer, cv, labelConverter])\r\n",
        "#cvModel = pipeline.fit(trainingData)\r\n",
        "\r\n",
        "pipeline = Pipeline(stages=[featureIndexer, lr, labelConverter]) \r\n",
        "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5, parallelism=10, seed=100)\r\n",
        "cvModel = cv.fit(trainingData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Use the new data(test data) for testing\r\n",
        "predictions = cvModel.transform(testData)\r\n",
        "\r\n",
        "predictions.select(\"features\", \"label\", \"probability\", \"predictedLabel\").show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Evaluate the best model\r\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\r\n",
        "print(\"The Accuracy for test set is {}\".format(evaluator.evaluate(predictions)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "predictionAndLabel = predictions.select(\"prediction\", \"indexedLabel\").rdd\r\n",
        "\r\n",
        "# Instantiate metrics object \r\n",
        "metricsMulti = MulticlassMetrics(predictionAndLabel)\r\n",
        "metricsBinary= BinaryClassificationMetrics(predictionAndLabel)\r\n",
        "# Overall statistics \r\n",
        "confusionMatrix = metricsMulti.confusionMatrix()\r\n",
        "precision = metricsMulti.precision(label=1) \r\n",
        "recall = metricsMulti.recall(label=1) \r\n",
        "f1Score = metricsMulti.fMeasure(0.0,1.0) \r\n",
        "print(\"Summary Stats\")\r\n",
        "print(\"Confusion Matrix = \\n %s\" % confusionMatrix)\r\n",
        "print(\"Precision = %s\" % precision) \r\n",
        "print(\"Recall = %s\" % recall) \r\n",
        "print(\"F1 Score = %s\" % f1Score) \r\n",
        "\r\n",
        "# Area under precision-recall curve \r\n",
        "print(\"Area under PR = %s\" % metricsBinary.areaUnderPR) \r\n",
        "# Area under ROC curve \r\n",
        "print(\"Area under ROC = %s\" % metricsBinary.areaUnderROC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",labelCol=\"indexedLabel\")\r\n",
        "print(\"The area under ROC for test set is {}\".format(evaluator.evaluate(predictions)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#  Decision Trees\r\n",
        "#Evaluate Decision Tree Algorithm\r\n",
        "# Create initial Decision Tree Model\r\n",
        "dt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\")\r\n",
        "\r\n",
        "# Train model with Training Data.\r\n",
        "dtModel = dt.fit(trainingData)\r\n",
        "\r\n",
        "# Make predictions on test data.\r\n",
        "predictions = dtModel.transform(testData)\r\n",
        "\r\n",
        "# Evaluate the model by computing the metrics. \r\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\r\n",
        "print(\"The Accuracy for test set is {}\".format(evaluator.evaluate(predictions)))\r\n",
        "\r\n",
        "print(\"===============================================\")\r\n",
        "\r\n",
        "predictionAndLabel = predictions.select(\"prediction\", \"indexedLabel\").rdd\r\n",
        "\r\n",
        "# Instantiate metrics object \r\n",
        "metricsMulti = MulticlassMetrics(predictionAndLabel)\r\n",
        "metricsBinary= BinaryClassificationMetrics(predictionAndLabel)\r\n",
        "# Overall statistics \r\n",
        "confusionMatrix = metricsMulti.confusionMatrix()\r\n",
        "precision = metricsMulti.precision(label=1) \r\n",
        "recall = metricsMulti.recall(label=1) \r\n",
        "f1Score = metricsMulti.fMeasure(0.0,1.0) \r\n",
        "print(\"Summary Stats\")\r\n",
        "print(\"Confusion Matrix = \\n %s\" % confusionMatrix)\r\n",
        "print(\"Precision = %s\" % precision) \r\n",
        "print(\"Recall = %s\" % recall) \r\n",
        "print(\"F1 Score = %s\" % f1Score) \r\n",
        "\r\n",
        "# Area under precision-recall curve \r\n",
        "print(\"Area under PR = %s\" % metricsBinary.areaUnderPR) \r\n",
        "# Area under ROC curve \r\n",
        "print(\"Area under ROC = %s\" % metricsBinary.areaUnderROC)\r\n",
        "\r\n",
        "print(\"===============================================\")\r\n",
        "\r\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",labelCol=\"indexedLabel\")\r\n",
        "print(\"The area under ROC for test set is {}\".format(evaluator.evaluate(predictions)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Hyperparameter tuning using 5-fold cross validation\r\n",
        "paramGrid = (ParamGridBuilder()\r\n",
        "             .addGrid(dt.maxDepth, [1, 2, 6, 10])\r\n",
        "             .addGrid(dt.maxBins, [20, 40, 80])\r\n",
        "             .build())\r\n",
        "\r\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",labelCol=\"indexedLabel\")\r\n",
        "\r\n",
        "pipeline = Pipeline(stages=[featureIndexer, dt, labelConverter]) \r\n",
        "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5, parallelism=10, seed=100)\r\n",
        "cvModel = cv.fit(trainingData)\r\n",
        "\r\n",
        "predictions = cvModel.transform(testData)\r\n",
        "\r\n",
        "# Evaluate the best model\r\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\r\n",
        "print(\"The Accuracy for test set is {}\".format(evaluator.evaluate(predictions)))\r\n",
        "\r\n",
        "print(\"===============================================\")\r\n",
        "\r\n",
        "predictionAndLabel = predictions.select(\"prediction\", \"indexedLabel\").rdd\r\n",
        "\r\n",
        "# Instantiate metrics object \r\n",
        "metricsMulti = MulticlassMetrics(predictionAndLabel)\r\n",
        "metricsBinary= BinaryClassificationMetrics(predictionAndLabel)\r\n",
        "# Overall statistics \r\n",
        "confusionMatrix = metricsMulti.confusionMatrix()\r\n",
        "precision = metricsMulti.precision(label=1) \r\n",
        "recall = metricsMulti.recall(label=1) \r\n",
        "f1Score = metricsMulti.fMeasure(0.0,1.0) \r\n",
        "print(\"Summary Stats\")\r\n",
        "print(\"Confusion Matrix = \\n %s\" % confusionMatrix)\r\n",
        "print(\"Precision = %s\" % precision) \r\n",
        "print(\"Recall = %s\" % recall) \r\n",
        "print(\"F1 Score = %s\" % f1Score) \r\n",
        "\r\n",
        "# Area under precision-recall curve \r\n",
        "print(\"Area under PR = %s\" % metricsBinary.areaUnderPR) \r\n",
        "# Area under ROC curve \r\n",
        "print(\"Area under ROC = %s\" % metricsBinary.areaUnderROC)\r\n",
        "\r\n",
        "print(\"===============================================\")\r\n",
        "\r\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",labelCol=\"indexedLabel\")\r\n",
        "print(\"The area under ROC for test set is {}\".format(evaluator.evaluate(predictions)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Random Forest\r\n",
        "#Evaluate Random Forest Algorithm\r\n",
        "# Create initial Random Forest Classifier\r\n",
        "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\")\r\n",
        "\r\n",
        "# Train model with Training Data.\r\n",
        "rfModel = rf.fit(trainingData)\r\n",
        "\r\n",
        "# Make predictions on test data.\r\n",
        "predictions = rfModel.transform(testData)\r\n",
        "\r\n",
        "# Evaluate the model by computing the metrics. \r\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\r\n",
        "print(\"The Accuracy for test set is {}\".format(evaluator.evaluate(predictions)))\r\n",
        "\r\n",
        "print(\"===============================================\")\r\n",
        "\r\n",
        "predictionAndLabel = predictions.select(\"prediction\", \"indexedLabel\").rdd\r\n",
        "\r\n",
        "# Instantiate metrics object \r\n",
        "metricsMulti = MulticlassMetrics(predictionAndLabel)\r\n",
        "metricsBinary= BinaryClassificationMetrics(predictionAndLabel)\r\n",
        "# Overall statistics \r\n",
        "confusionMatrix = metricsMulti.confusionMatrix()\r\n",
        "precision = metricsMulti.precision(label=1) \r\n",
        "recall = metricsMulti.recall(label=1) \r\n",
        "f1Score = metricsMulti.fMeasure(0.0,1.0) \r\n",
        "print(\"Summary Stats\")\r\n",
        "print(\"Confusion Matrix = \\n %s\" % confusionMatrix)\r\n",
        "print(\"Precision = %s\" % precision) \r\n",
        "print(\"Recall = %s\" % recall) \r\n",
        "print(\"F1 Score = %s\" % f1Score) \r\n",
        "\r\n",
        "# Area under precision-recall curve \r\n",
        "print(\"Area under PR = %s\" % metricsBinary.areaUnderPR) \r\n",
        "# Area under ROC curve \r\n",
        "print(\"Area under ROC = %s\" % metricsBinary.areaUnderROC)\r\n",
        "\r\n",
        "print(\"===============================================\")\r\n",
        "\r\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",labelCol=\"indexedLabel\")\r\n",
        "print(\"The area under ROC for test set is {}\".format(evaluator.evaluate(predictions)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Hyperparameter tuning using 5-fold cross validation\r\n",
        "paramGrid = (ParamGridBuilder()\r\n",
        "             .addGrid(rf.maxDepth, [2, 4, 6])\r\n",
        "             .addGrid(rf.maxBins, [20, 60])\r\n",
        "             .addGrid(rf.numTrees, [5, 20])\r\n",
        "             .build())\r\n",
        "\r\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",labelCol=\"indexedLabel\")\r\n",
        "\r\n",
        "pipeline = Pipeline(stages=[featureIndexer, rf, labelConverter]) \r\n",
        "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5, parallelism=10, seed=100)\r\n",
        "cvModel = cv.fit(trainingData)\r\n",
        "\r\n",
        "predictions = cvModel.transform(testData)\r\n",
        "\r\n",
        "# Evaluate the best model\r\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\r\n",
        "print(\"The Accuracy for test set is {}\".format(evaluator.evaluate(predictions)))\r\n",
        "\r\n",
        "print(\"===============================================\")\r\n",
        "\r\n",
        "predictionAndLabel = predictions.select(\"prediction\", \"indexedLabel\").rdd\r\n",
        "\r\n",
        "# Instantiate metrics object \r\n",
        "metricsMulti = MulticlassMetrics(predictionAndLabel)\r\n",
        "metricsBinary= BinaryClassificationMetrics(predictionAndLabel)\r\n",
        "# Overall statistics \r\n",
        "confusionMatrix = metricsMulti.confusionMatrix()\r\n",
        "precision = metricsMulti.precision(label=1) \r\n",
        "recall = metricsMulti.recall(label=1) \r\n",
        "f1Score = metricsMulti.fMeasure(0.0,1.0) \r\n",
        "print(\"Summary Stats\")\r\n",
        "print(\"Confusion Matrix = \\n %s\" % confusionMatrix)\r\n",
        "print(\"Precision = %s\" % precision) \r\n",
        "print(\"Recall = %s\" % recall) \r\n",
        "print(\"F1 Score = %s\" % f1Score) \r\n",
        "\r\n",
        "# Area under precision-recall curve\r\n",
        "print(\"Area under PR = %s\" % metricsBinary.areaUnderPR) \r\n",
        "# Area under ROC curve \r\n",
        "print(\"Area under ROC = %s\" % metricsBinary.areaUnderROC)\r\n",
        "\r\n",
        "print(\"===============================================\")\r\n",
        "\r\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",labelCol=\"indexedLabel\")\r\n",
        "print(\"The area under ROC for test set is {}\".format(evaluator.evaluate(predictions)))\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from onnxmltools import convert_sparkml\r\n",
        "from onnxmltools.convert.sparkml.utils import buildInitialTypesSimple\r\n",
        "initial_types = buildInitialTypesSimple(test_df.drop(\"label\"))\r\n",
        "onnx_model = convert_sparkml(model, 'Churn prediction model', initial_types, spark_session = spark)\r\n",
        "\r\n",
        "from onnxmltools import convert_sparkml\r\n",
        "from onnxmltools.utils import save_model\r\n",
        "model_onnx = convert_sparkml(pipelineModel, 'churn prediction model', initial_types)\r\n",
        "model_onnx"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}